{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8b4f4bc-f7b3-493f-be83-24eb50dd7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf1303de-806e-4f40-a746-488ec8051b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROWS = 4\n",
    "NUM_COLUMNS = 3\n",
    "STEP_COST = -0.04\n",
    "GAMMA = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa5af542-1579-4efb-87d8-3f9824bfdb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = [ (2, 1) ]\n",
    "T = [ (2, 1), (0, 1), (0, 2) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb363f87-43cf-4521-a30d-b3ad760a785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(\n",
    "    state: (int, int)\n",
    ") -> bool:\n",
    "\n",
    "    row = state[0]\n",
    "    col = state[1]\n",
    "\n",
    "    if not 0 <= row < NUM_ROWS:\n",
    "        return False\n",
    "\n",
    "    if not 0 <= col < NUM_COLUMNS:\n",
    "        return False\n",
    "\n",
    "    if state in W:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e8f7817-62ed-4b7e-8de9-3d654ff74ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_actions(\n",
    "    row: int,\n",
    "    col: int,\n",
    "    p: float\n",
    ") -> dict[str, dict[(int, int), float]]:\n",
    "\n",
    "    acc = p\n",
    "    fail = (1 - p) / 2\n",
    "\n",
    "    actions = {}\n",
    "    actions['up'] = { (row - 1, col): acc, (row, col - 1): fail, (row, col + 1): fail }\n",
    "    actions['down'] = { (row + 1, col): acc, (row, col - 1): fail, (row, col + 1): fail }\n",
    "    actions['left'] = { (row, col - 1): acc, (row - 1, col): fail, (row + 1, col): fail }\n",
    "    actions['right'] = { (row, col + 1): acc, (row - 1, col): fail, (row + 1, col): fail }\n",
    "\n",
    "    return constrain_actions(row, col, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2793aec9-7163-4f52-822d-6953c6390a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrain_actions(\n",
    "    row: int,\n",
    "    col: int,\n",
    "    actions: dict[str, dict[(int, int), float]]\n",
    ") -> dict[str, dict[(int, int), float]]:\n",
    "\n",
    "    actions_new = {}\n",
    "    for action, transition in actions.items():\n",
    "        residue = 0\n",
    "        transitions_new = {}\n",
    "        for state, prob in transition.items():\n",
    "            if is_valid(state):\n",
    "                transitions_new[state] = prob\n",
    "            else:\n",
    "                residue += prob\n",
    "        if residue != 0:\n",
    "            transitions_new[(row, col)] = residue\n",
    "        actions_new[action] = transitions_new\n",
    "\n",
    "    return actions_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf7c45f4-0bde-42fc-8ec2-67ef30cd13bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_actions(\n",
    "    U: list[list[int]],\n",
    "    actions: dict[str, dict[(int, int), float]]\n",
    ") -> dict[str, float]:\n",
    "\n",
    "    evals = {}\n",
    "    for action, transition in actions.items():\n",
    "        value = 0\n",
    "        for state, prob in transition.items():\n",
    "            value += prob * U[state[0]][state[1]]\n",
    "        evals[action] = value\n",
    "\n",
    "    return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab78791c-efee-449d-a505-a7e8c56fd5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_action(\n",
    "    evals: dict[str, float]\n",
    ") -> (str, float):\n",
    "\n",
    "    opt_value = -1e30\n",
    "    opt_strategy = None\n",
    "    for strategy, value in evals.items():\n",
    "        if value > opt_value:\n",
    "            opt_value = value\n",
    "            opt_strategy = strategy\n",
    "\n",
    "    return opt_strategy, opt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f7f73e4-544b-4c1f-b6df-04b4cd86e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(\n",
    "    p: float,\n",
    "    log: bool = False\n",
    ") -> (np.ndarray, np.ndarray):\n",
    "\n",
    "    U = np.zeros((NUM_ROWS, NUM_COLUMNS))\n",
    "    P = np.full((NUM_ROWS, NUM_COLUMNS), 'none', dtype='object') \n",
    "    U[0][1] = -1\n",
    "    U[0][2] = 1\n",
    "\n",
    "    k = 0\n",
    "    while True:\n",
    "        U_new = np.copy(U)\n",
    "        P_new = np.copy(P)\n",
    "\n",
    "        for i, U_i in enumerate(U):\n",
    "            for j, U_ij in enumerate(U_i):\n",
    "                if (i, j) not in T:\n",
    "                    actions = generate_actions(i, j, p)\n",
    "                    evals = evaluate_actions(U, actions)\n",
    "                    action, val = optimal_action(evals)\n",
    "                    U_new[i][j] = STEP_COST + GAMMA * val\n",
    "                    P_new[i][j] = action\n",
    "\n",
    "        if (np.absolute(U_new - U) <= 1e-4).all():\n",
    "            break\n",
    "        if log:\n",
    "            print(U_new, end='\\n\\n')\n",
    "\n",
    "        k = k + 1\n",
    "        U = U_new\n",
    "        P = P_new\n",
    "\n",
    "    return U, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "776251cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04 -1.    1.  ]\n",
      " [-0.04 -0.04  0.62]\n",
      " [-0.04  0.   -0.04]\n",
      " [-0.04 -0.04 -0.04]]\n",
      "\n",
      "[[-0.08 -1.    1.  ]\n",
      " [-0.08  0.23  0.71]\n",
      " [-0.08  0.    0.36]\n",
      " [-0.08 -0.08 -0.08]]\n",
      "\n",
      "[[-0.11 -1.    1.  ]\n",
      " [ 0.09  0.32  0.76]\n",
      " [-0.11  0.    0.53]\n",
      " [-0.11 -0.11  0.18]]\n",
      "\n",
      "[[-0.12 -1.    1.  ]\n",
      " [ 0.14  0.37  0.78]\n",
      " [-0.01  0.    0.62]\n",
      " [-0.15  0.05  0.33]]\n",
      "\n",
      "[[-0.11 -1.    1.  ]\n",
      " [ 0.19  0.39  0.79]\n",
      " [ 0.05  0.    0.65]\n",
      " [-0.03  0.19  0.42]]\n",
      "\n",
      "[[-0.07 -1.    1.  ]\n",
      " [ 0.21  0.4   0.79]\n",
      " [ 0.1   0.    0.67]\n",
      " [ 0.09  0.3   0.48]]\n",
      "\n",
      "[[-0.05 -1.    1.  ]\n",
      " [ 0.23  0.4   0.79]\n",
      " [ 0.13  0.    0.68]\n",
      " [ 0.18  0.36  0.52]]\n",
      "\n",
      "[[-0.04 -1.    1.  ]\n",
      " [ 0.24  0.4   0.8 ]\n",
      " [ 0.15  0.    0.68]\n",
      " [ 0.25  0.41  0.54]]\n",
      "\n",
      "[[-0.03 -1.    1.  ]\n",
      " [ 0.24  0.4   0.8 ]\n",
      " [ 0.17  0.    0.68]\n",
      " [ 0.29  0.43  0.55]]\n",
      "\n",
      "[[-0.02 -1.    1.  ]\n",
      " [ 0.25  0.4   0.8 ]\n",
      " [ 0.2   0.    0.68]\n",
      " [ 0.31  0.45  0.55]]\n",
      "\n",
      "[[-0.02 -1.    1.  ]\n",
      " [ 0.25  0.4   0.8 ]\n",
      " [ 0.22  0.    0.68]\n",
      " [ 0.33  0.46  0.56]]\n",
      "\n",
      "[[-0.02 -1.    1.  ]\n",
      " [ 0.26  0.4   0.8 ]\n",
      " [ 0.24  0.    0.68]\n",
      " [ 0.34  0.46  0.56]]\n",
      "\n",
      "[[-0.01 -1.    1.  ]\n",
      " [ 0.26  0.4   0.8 ]\n",
      " [ 0.26  0.    0.68]\n",
      " [ 0.35  0.46  0.56]]\n",
      "\n",
      "[[-0.01 -1.    1.  ]\n",
      " [ 0.26  0.4   0.8 ]\n",
      " [ 0.27  0.    0.68]\n",
      " [ 0.35  0.46  0.56]]\n",
      "\n",
      "[[-0.01 -1.    1.  ]\n",
      " [ 0.27  0.4   0.8 ]\n",
      " [ 0.27  0.    0.68]\n",
      " [ 0.36  0.47  0.56]]\n",
      "\n",
      "[[-0.01 -1.    1.  ]\n",
      " [ 0.27  0.4   0.8 ]\n",
      " [ 0.28  0.    0.68]\n",
      " [ 0.36  0.47  0.56]]\n",
      "\n",
      "[[-0.01 -1.    1.  ]\n",
      " [ 0.27  0.4   0.8 ]\n",
      " [ 0.28  0.    0.68]\n",
      " [ 0.36  0.47  0.56]]\n",
      "\n",
      "[[-0.01 -1.    1.  ]\n",
      " [ 0.27  0.4   0.8 ]\n",
      " [ 0.28  0.    0.68]\n",
      " [ 0.36  0.47  0.56]]\n",
      "\n",
      "[[-0.01 -1.    1.  ]\n",
      " [ 0.27  0.4   0.8 ]\n",
      " [ 0.28  0.    0.68]\n",
      " [ 0.36  0.47  0.56]]\n",
      "\n",
      "[[-0.01 -1.    1.  ]\n",
      " [ 0.27  0.4   0.8 ]\n",
      " [ 0.28  0.    0.68]\n",
      " [ 0.36  0.47  0.56]]\n",
      "\n",
      "[[-0.   -1.    1.  ]\n",
      " [ 0.27  0.4   0.8 ]\n",
      " [ 0.28  0.    0.68]\n",
      " [ 0.36  0.47  0.56]]\n",
      "\n",
      "[[-0.   -1.    1.  ]\n",
      " [ 0.27  0.4   0.8 ]\n",
      " [ 0.28  0.    0.68]\n",
      " [ 0.36  0.47  0.56]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utility, policy = value_iteration(0.7, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71beed99-9d60-4019-b472-5f5f5b143045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p = 0.1\n",
      "[[ 0.08 -1.    1.  ]\n",
      " [ 0.18  0.32  0.59]\n",
      " [ 0.02  0.    0.33]\n",
      " [-0.04  0.04  0.22]]\n",
      "[['left' 'none' 'none']\n",
      " ['up' 'down' 'right']\n",
      " ['left' 'none' 'left']\n",
      " ['up' 'up' 'right']]\n",
      "\n",
      "p = 0.2\n",
      "[[-0.02 -1.    1.  ]\n",
      " [ 0.08  0.25  0.55]\n",
      " [-0.06  0.    0.28]\n",
      " [-0.09 -0.02  0.16]]\n",
      "[['left' 'none' 'none']\n",
      " ['up' 'down' 'right']\n",
      " ['left' 'none' 'left']\n",
      " ['up' 'up' 'right']]\n",
      "\n",
      "p = 0.3\n",
      "[[-0.08 -1.    1.  ]\n",
      " [ 0.03  0.23  0.58]\n",
      " [-0.06  0.    0.38]\n",
      " [-0.04  0.07  0.22]]\n",
      "[['left' 'none' 'none']\n",
      " ['down' 'down' 'right']\n",
      " ['left' 'none' 'up']\n",
      " ['down' 'right' 'right']]\n",
      "\n",
      "p = 0.4\n",
      "[[-0.14 -1.    1.  ]\n",
      " [-0.02  0.2   0.59]\n",
      " [-0.08  0.    0.43]\n",
      " [-0.01  0.13  0.25]]\n",
      "[['left' 'none' 'none']\n",
      " ['down' 'down' 'right']\n",
      " ['left' 'none' 'up']\n",
      " ['down' 'right' 'right']]\n",
      "\n",
      "p = 0.5\n",
      "[[-0.1  -1.    1.  ]\n",
      " [ 0.05  0.24  0.64]\n",
      " [ 0.    0.    0.51]\n",
      " [ 0.09  0.23  0.33]]\n",
      "[['left' 'none' 'none']\n",
      " ['right' 'down' 'up']\n",
      " ['down' 'none' 'up']\n",
      " ['right' 'right' 'up']]\n",
      "\n",
      "p = 0.6\n",
      "[[-0.05 -1.    1.  ]\n",
      " [ 0.14  0.29  0.72]\n",
      " [ 0.15  0.    0.6 ]\n",
      " [ 0.24  0.35  0.46]]\n",
      "[['left' 'none' 'none']\n",
      " ['right' 'down' 'up']\n",
      " ['down' 'none' 'up']\n",
      " ['right' 'right' 'up']]\n",
      "\n",
      "p = 0.7\n",
      "[[-0.   -1.    1.  ]\n",
      " [ 0.27  0.4   0.8 ]\n",
      " [ 0.28  0.    0.68]\n",
      " [ 0.36  0.47  0.56]]\n",
      "[['down' 'none' 'none']\n",
      " ['right' 'right' 'up']\n",
      " ['down' 'none' 'up']\n",
      " ['right' 'right' 'up']]\n",
      "\n",
      "p = 0.8\n",
      "[[ 0.23 -1.    1.  ]\n",
      " [ 0.45  0.57  0.86]\n",
      " [ 0.39  0.    0.75]\n",
      " [ 0.46  0.56  0.65]]\n",
      "[['down' 'none' 'none']\n",
      " ['right' 'right' 'up']\n",
      " ['down' 'none' 'up']\n",
      " ['right' 'right' 'up']]\n",
      "\n",
      "p = 0.9\n",
      "[[ 0.46 -1.    1.  ]\n",
      " [ 0.61  0.71  0.89]\n",
      " [ 0.53  0.    0.8 ]\n",
      " [ 0.54  0.62  0.7 ]]\n",
      "[['down' 'none' 'none']\n",
      " ['right' 'right' 'up']\n",
      " ['up' 'none' 'up']\n",
      " ['right' 'right' 'up']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    p = i / 10\n",
    "    utility, policy = value_iteration(p)\n",
    "    print('p =', p)\n",
    "    print(utility)\n",
    "    print(policy)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f8c06",
   "metadata": {},
   "source": [
    "### Analysis of Policy\n",
    "\n",
    "- In case of high accuracy in actions, i.e. high value of p, the policy is directed towards the path leading to the reward state. The bottom-left state, i.e. (3, 0) which has two paths to reach the same, prefers the one that avoids moving close to the penalty state. The same is noticed with (2, 0) as p decreases slightly.\n",
    "\n",
    "- As the accuracy drops below a certain threshold, the value iteration method adopts a precautious mindset that weighs higher the odds of falling into the penalty state. This can be observed with the states (0, 0) and (1, 1), that prescribe action exactly opposite to the penalty state, resulting in zero probability of transitioning to the penalty state. With further drop, (1, 0) and (2, 0) also follow suit.\n",
    "\n",
    "- When inaccurate movements start to dominate, i.e. the probability of perpendicular movements shoot up, we notice the policy starts prescribing actions perpendicular to what it suggested earlier. For example, the state below the reward state, i.e. (1, 2) suggests moving right as the optimal action, instead of the perpendicular up. Similarly, the bottom-left state, i.e. (3, 0) suggests moving up, instead of right."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
